Step 1: Install Java (Required for Kafka & Spark)
Check if Java is installed:

bash
Copy
Edit

java -version

If not installed, install it using Homebrew:

bash
Copy
Edit

brew install openjdk

Step 2: Install Apache Kafka on Mac
1. Install Kafka using Homebrew
bash
Copy
Edit

brew install kafka

2. Start Zookeeper (Required for Kafka)
bash
Copy
Edit

zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties

3. Start Kafka
bash
Copy
Edit

kafka-server-start /opt/homebrew/etc/kafka/server.properties

✅ Now Kafka is running on your Mac!

Step 3: Install Apache Spark on Mac
1. Install Spark using Homebrew
bash
Copy
Edit

brew install apache-spark

2. Verify Spark Installation
bash
Copy
Edit

spark-shell

If Spark runs without errors, it is installed correctly.

✅ Now Spark is installed on your Mac!

Step 4: Install Python Libraries
Run this in Terminal or Jupyter Notebook:

bash
Copy
Edit

pip install kafka-python pyspark delta-spark

✅ Now, you can use Kafka and Spark in Python.

Step 2: Kafka Producer (Publishing Data to Kafka)
This script reads a CSV file using Spark, processes it, and then publishes messages to a Kafka topic.

python
Copy
Edit

from pyspark.sql import SparkSession
from kafka import KafkaProducer
import json
import time

# Initialize Spark Session
spark = SparkSession.builder.appName("KafkaProducer").getOrCreate()

# Read CSV File
df = spark.read.option("header", True).csv("wind_power_data.csv")

# Convert DataFrame to List of Dictionaries
data = df.toPandas().to_dict(orient="records")

# Kafka Configuration
KAFKA_BROKER = "localhost:9092"  # Update with your Kafka broker
KAFKA_TOPIC = "wind_power_data"

# Initialize Kafka Producer
producer = KafkaProducer(
    bootstrap_servers=KAFKA_BROKER,
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

# Publish Messages to Kafka
for record in data:
    producer.send(KAFKA_TOPIC, value=record)
    time.sleep(0.5)  # Small delay to simulate streaming

print("Data published to Kafka successfully!")
producer.flush()
producer.close()

✅ This script reads a CSV file, converts it to JSON, and publishes records to a Kafka topic.

Step 3: Kafka Subscriber (Reading Data from Kafka and Writing to Delta Table)
This script subscribes to Kafka, reads the messages using Spark Structured Streaming, and writes data to a Delta table.

python
Copy
Edit

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType
from delta import *

# Initialize Spark Session with Delta Support
spark = SparkSession.builder \
    .appName("KafkaSubscriber") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Kafka Configuration
KAFKA_BROKER = "localhost:9092"
KAFKA_TOPIC = "wind_power_data"

# Define Schema for Incoming Data
schema = StructType([
    StructField("signal_date", StringType(), True),
    StructField("signal_ts", TimestampType(), True),
    StructField("LV ActivePower (kW)", FloatType(), True),
    StructField("Wind Speed (m/s)", FloatType(), True),
    StructField("Theoretical_Power_Curve (KWh)", FloatType(), True),
    StructField("Wind Direction (°)", FloatType(), True)
])

# Read Data from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BROKER) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "earliest") \
    .load()

# Deserialize JSON Messages
df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# Write to Delta Table
df.writeStream \
    .format("delta") \
    .option("path", "delta_table_path") \
    .option("checkpointLocation", "delta_checkpoint") \
    .outputMode("append") \
    .start() \
    .awaitTermination()

✅ This script reads data from Kafka, applies schema, and writes it to a Delta table.
